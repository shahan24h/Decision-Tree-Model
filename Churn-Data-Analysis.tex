% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Churn Data Analysis},
  pdfauthor={Shahan Ahmed},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Churn Data Analysis}
\author{Shahan Ahmed}
\date{2024-05-11}

\begin{document}
\maketitle

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'ggplot2' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
## Warning: package 'lubridate' was built under R version 4.3.2
\end{verbatim}

\begin{verbatim}
## -- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --
## v dplyr     1.1.2     v readr     2.1.4
## v forcats   1.0.0     v stringr   1.5.0
## v ggplot2   3.4.4     v tibble    3.2.1
## v lubridate 1.9.3     v tidyr     1.3.0
## v purrr     1.0.2     
## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
## i Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(dplyr)}
\FunctionTok{library}\NormalTok{(foreign)}
\FunctionTok{library}\NormalTok{(rpart)}
\FunctionTok{library}\NormalTok{(rpart.plot)}
\FunctionTok{library}\NormalTok{(haven)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Warning: package 'haven' was built under R version 4.3.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidymodels)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## -- Attaching packages -------------------------------------- tidymodels 1.1.1 --
## v broom        1.0.5     v rsample      1.2.0
## v dials        1.2.0     v tune         1.1.2
## v infer        1.0.5     v workflows    1.1.3
## v modeldata    1.2.0     v workflowsets 1.0.1
## v parsnip      1.1.1     v yardstick    1.2.0
## v recipes      1.0.8     
## -- Conflicts ----------------------------------------- tidymodels_conflicts() --
## x scales::discard() masks purrr::discard()
## x dplyr::filter()   masks stats::filter()
## x recipes::fixed()  masks stringr::fixed()
## x dplyr::lag()      masks stats::lag()
## x dials::prune()    masks rpart::prune()
## x yardstick::spec() masks readr::spec()
## x recipes::step()   masks stats::step()
## * Use tidymodels_prefer() to resolve common conflicts.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rattle)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: bitops
## Rattle: A free graphical interface for data science with R.
## Version 5.5.1 Copyright (c) 2006-2021 Togaware Pty Ltd.
## Type 'rattle()' to shake, rattle, and roll your data.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(caret)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Loading required package: lattice
## 
## Attaching package: 'caret'
## 
## The following objects are masked from 'package:yardstick':
## 
##     precision, recall, sensitivity, specificity
## 
## The following object is masked from 'package:purrr':
## 
##     lift
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(naniar)}
\end{Highlighting}
\end{Shaded}

\#Step 1: Import the data

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn }\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"Churn.csv"}\NormalTok{, }\AttributeTok{header=} \ConstantTok{TRUE}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{stringsAsFactors =}\NormalTok{ T) }
\end{Highlighting}
\end{Shaded}

\#Step 2: Clean the dataset

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn }\OtherTok{\textless{}{-}}\NormalTok{ churn }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Churn, gender, SeniorCitizen, Partner, Dependents, tenure, MonthlyCharges)}
\NormalTok{na.omit}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## function (object, ...) 
## UseMethod("na.omit")
## <bytecode: 0x000002dbae361c60>
## <environment: namespace:stats>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{str}\NormalTok{(churn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    7043 obs. of  7 variables:
##  $ Churn         : Factor w/ 2 levels "No","Yes": 1 1 2 1 2 2 1 1 2 1 ...
##  $ gender        : Factor w/ 2 levels "Female","Male": 1 2 2 2 1 1 2 1 1 2 ...
##  $ SeniorCitizen : int  0 0 0 0 0 0 0 0 0 0 ...
##  $ Partner       : Factor w/ 2 levels "No","Yes": 2 1 1 1 1 1 1 1 2 1 ...
##  $ Dependents    : Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 2 1 1 2 ...
##  $ tenure        : int  1 34 2 45 2 8 22 10 28 62 ...
##  $ MonthlyCharges: num  29.9 57 53.9 42.3 70.7 ...
\end{verbatim}

\#Found that `seniorCitizen' is integer. Let's code this into factor

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{churn1}\OtherTok{\textless{}{-}}\NormalTok{churn }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{SeniorCitizen =} \FunctionTok{recode\_factor}\NormalTok{(SeniorCitizen,}
                                       \StringTok{\textquotesingle{}1\textquotesingle{}}\OtherTok{=}\StringTok{"Yes"}\NormalTok{,}
                                       \StringTok{\textquotesingle{}0\textquotesingle{}}\OtherTok{=}\StringTok{"No"}\NormalTok{))}

\NormalTok{churn1 }\OtherTok{\textless{}{-}}\NormalTok{ churn1 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{TenureCategory =} \FunctionTok{cut}\NormalTok{(tenure,}
                              \AttributeTok{breaks =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{12}\NormalTok{, }\DecValTok{36}\NormalTok{, }\ConstantTok{Inf}\NormalTok{),}
                              \AttributeTok{right =} \ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
\FunctionTok{mutate}\NormalTok{(}\AttributeTok{TenureCategory=} \FunctionTok{as.factor}\NormalTok{(TenureCategory))}

\NormalTok{churn1 }\OtherTok{\textless{}{-}}\NormalTok{ churn1 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{tenure)}


\FunctionTok{str}\NormalTok{(churn1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    7043 obs. of  7 variables:
##  $ Churn         : Factor w/ 2 levels "No","Yes": 1 1 2 1 2 2 1 1 2 1 ...
##  $ gender        : Factor w/ 2 levels "Female","Male": 1 2 2 2 1 1 2 1 1 2 ...
##  $ SeniorCitizen : Factor w/ 2 levels "Yes","No": 2 2 2 2 2 2 2 2 2 2 ...
##  $ Partner       : Factor w/ 2 levels "No","Yes": 2 1 1 1 1 1 1 1 2 1 ...
##  $ Dependents    : Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 2 1 1 2 ...
##  $ MonthlyCharges: num  29.9 57 53.9 42.3 70.7 ...
##  $ TenureCategory: Factor w/ 3 levels "(0,12]","(12,36]",..: 1 2 1 3 1 1 2 1 2 3 ...
\end{verbatim}

\#Step 3: Create train/test set

\#Using the ``tidymodels'' package to split the data set into training
(70\%) and testing set (30\%); then build a decision tree for the target
``Churn'' by including all feature variables and setting the cp value to
0.01.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{data\_split }\OtherTok{\textless{}{-}} \FunctionTok{initial\_split}\NormalTok{(churn1, }\AttributeTok{prop=}\FloatTok{0.7}\NormalTok{, }\AttributeTok{strata =}\NormalTok{ Churn)}

\NormalTok{traindf}\OtherTok{\textless{}{-}} \FunctionTok{training}\NormalTok{(data\_split)}
\NormalTok{testdf}\OtherTok{\textless{}{-}} \FunctionTok{testing}\NormalTok{(data\_split)}

\NormalTok{mytree }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(Churn}\SpecialCharTok{\textasciitilde{}}\NormalTok{., }\AttributeTok{data =}\NormalTok{ traindf, }\AttributeTok{method=}\StringTok{"class"}\NormalTok{, }\AttributeTok{minsplit=}\DecValTok{2}\NormalTok{, }\AttributeTok{minbucket=}\DecValTok{1}\NormalTok{, }\AttributeTok{cp=}\FloatTok{0.01}\NormalTok{)}


\FunctionTok{rpart.plot}\NormalTok{(mytree, }\AttributeTok{type=}\DecValTok{1}\NormalTok{, }\AttributeTok{extra =} \DecValTok{101}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Churn-Data-Analysis_files/figure-latex/unnamed-chunk-5-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fancyRpartPlot}\NormalTok{(mytree)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Churn-Data-Analysis_files/figure-latex/unnamed-chunk-5-2.pdf}
There are 4 feature variables used in this tree, tenure and Monthly
charges. There are 7 leaf nodes

\#Next Step: According to the tree grown in step 1, if ``you'' have been
with the company for over 17 months, what is the probability that you
``churn?''

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rpart.plot}\NormalTok{(mytree, }\AttributeTok{type=}\DecValTok{1}\NormalTok{, }\AttributeTok{extra=}\StringTok{"auto"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Churn-Data-Analysis_files/figure-latex/unnamed-chunk-6-1.pdf}
We found that probability of yes ``churning''is 0.17\%

\#Now changing our parameter, and Set cp = 0.005 to grow another tree.

Let's find out that, if ``I'' have been with the company for 12 months
and is paying \$70 monthly charge, what is the probability that ``I''
``churn?''

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mytree2 }\OtherTok{\textless{}{-}} \FunctionTok{rpart}\NormalTok{(Churn}\SpecialCharTok{\textasciitilde{}}\NormalTok{gender}\SpecialCharTok{+}\NormalTok{SeniorCitizen}\SpecialCharTok{+}\NormalTok{Partner}\SpecialCharTok{+}\NormalTok{Dependents}\SpecialCharTok{+}\NormalTok{TenureCategory}\SpecialCharTok{+}\NormalTok{MonthlyCharges,}
              \AttributeTok{data=}\NormalTok{traindf,}
              \AttributeTok{method=}\StringTok{"class"}\NormalTok{,}
              \AttributeTok{minsplit=}\DecValTok{2}\NormalTok{,}
              \AttributeTok{minbucket=}\DecValTok{1}\NormalTok{,}
              \AttributeTok{cp=}\FloatTok{0.005}\NormalTok{)}

\FunctionTok{rpart.plot}\NormalTok{(mytree2, }\AttributeTok{type=}\DecValTok{1}\NormalTok{, }\AttributeTok{extra=}\StringTok{"auto"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Churn-Data-Analysis_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fancyRpartPlot}\NormalTok{(mytree2) }
\end{Highlighting}
\end{Shaded}

\includegraphics{Churn-Data-Analysis_files/figure-latex/unnamed-chunk-7-2.pdf}

\#3.The probability of you ``churning''is 0.32\%

Obtain the Confusion Matrices for the two tree models with cp=0.01 and
cp=0.005 on the training data only. Based on the differences of the
indexes of ``accuracy'', ``sensitivity,'' and ``specificity'' in the two
models, which one is better?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.pred}\OtherTok{\textless{}{-}}\FunctionTok{predict}\NormalTok{(mytree,traindf, }\AttributeTok{type=}\StringTok{"class"}\NormalTok{)}

\FunctionTok{confusionMatrix}\NormalTok{(train.pred, traindf}\SpecialCharTok{$}\NormalTok{Churn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  3407  853
##        Yes  214  455
##                                          
##                Accuracy : 0.7835         
##                  95% CI : (0.7718, 0.795)
##     No Information Rate : 0.7346         
##     P-Value [Acc > NIR] : 1.144e-15      
##                                          
##                   Kappa : 0.3421         
##                                          
##  Mcnemar's Test P-Value : < 2.2e-16      
##                                          
##             Sensitivity : 0.9409         
##             Specificity : 0.3479         
##          Pos Pred Value : 0.7998         
##          Neg Pred Value : 0.6801         
##              Prevalence : 0.7346         
##          Detection Rate : 0.6912         
##    Detection Prevalence : 0.8643         
##       Balanced Accuracy : 0.6444         
##                                          
##        'Positive' Class : No             
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{train.pred2}\OtherTok{\textless{}{-}}\FunctionTok{predict}\NormalTok{(mytree2, traindf, }\AttributeTok{type=}\StringTok{"class"}\NormalTok{)}

\FunctionTok{confusionMatrix}\NormalTok{(train.pred2, traindf}\SpecialCharTok{$}\NormalTok{Churn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  3407  853
##        Yes  214  455
##                                          
##                Accuracy : 0.7835         
##                  95% CI : (0.7718, 0.795)
##     No Information Rate : 0.7346         
##     P-Value [Acc > NIR] : 1.144e-15      
##                                          
##                   Kappa : 0.3421         
##                                          
##  Mcnemar's Test P-Value : < 2.2e-16      
##                                          
##             Sensitivity : 0.9409         
##             Specificity : 0.3479         
##          Pos Pred Value : 0.7998         
##          Neg Pred Value : 0.6801         
##              Prevalence : 0.7346         
##          Detection Rate : 0.6912         
##    Detection Prevalence : 0.8643         
##       Balanced Accuracy : 0.6444         
##                                          
##        'Positive' Class : No             
## 
\end{verbatim}

\#4.Two models are same in terms of accuracy, sensitivity, and
specifically.

Obtain the Confusion Matrices for the two tree models with cp=0.01 and
cp=0.005 on the testing data.

Also, let's compare the results with what we obtain in step last step,

is there evediance of model over-fitting?

Which tree(s) should you keep for decision making in the future?
(Provide evidence and make your own arguments).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.pred}\OtherTok{\textless{}{-}}\FunctionTok{predict}\NormalTok{(mytree,testdf,}\AttributeTok{type=}\StringTok{"class"}\NormalTok{)}

\FunctionTok{confusionMatrix}\NormalTok{(test.pred, testdf}\SpecialCharTok{$}\NormalTok{Churn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  1458  365
##        Yes   95  196
##                                           
##                Accuracy : 0.7824          
##                  95% CI : (0.7642, 0.7998)
##     No Information Rate : 0.7346          
##     P-Value [Acc > NIR] : 2.197e-07       
##                                           
##                   Kappa : 0.3406          
##                                           
##  Mcnemar's Test P-Value : < 2.2e-16       
##                                           
##             Sensitivity : 0.9388          
##             Specificity : 0.3494          
##          Pos Pred Value : 0.7998          
##          Neg Pred Value : 0.6735          
##              Prevalence : 0.7346          
##          Detection Rate : 0.6897          
##    Detection Prevalence : 0.8623          
##       Balanced Accuracy : 0.6441          
##                                           
##        'Positive' Class : No              
## 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{test.pred2}\OtherTok{\textless{}{-}}\FunctionTok{predict}\NormalTok{(mytree2,testdf,}\AttributeTok{type=}\StringTok{"class"}\NormalTok{)}

\FunctionTok{confusionMatrix}\NormalTok{(test.pred2, testdf}\SpecialCharTok{$}\NormalTok{Churn)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   No  Yes
##        No  1458  365
##        Yes   95  196
##                                           
##                Accuracy : 0.7824          
##                  95% CI : (0.7642, 0.7998)
##     No Information Rate : 0.7346          
##     P-Value [Acc > NIR] : 2.197e-07       
##                                           
##                   Kappa : 0.3406          
##                                           
##  Mcnemar's Test P-Value : < 2.2e-16       
##                                           
##             Sensitivity : 0.9388          
##             Specificity : 0.3494          
##          Pos Pred Value : 0.7998          
##          Neg Pred Value : 0.6735          
##              Prevalence : 0.7346          
##          Detection Rate : 0.6897          
##    Detection Prevalence : 0.8623          
##       Balanced Accuracy : 0.6441          
##                                           
##        'Positive' Class : No              
## 
\end{verbatim}

\#5.overfitting observed in both trees, as they are identical to each
other.

Lets see that the variable ``SeniorCitizen'' leads to information gain
as a feature (variable) in a tree model.

\end{document}
